---
title: LAWT:linear algeobra with Transformers
date: 2026-02-09 00:41:37
categories:
  - 项目
tags:
  - 项目
  - 视觉
  - RM
---

# LAWT 开发日志

目前是周日下午，距离我产生这个想法已过去五天有余，终于可以说大致完工了。

## 项目背景

项目的灵感来源于周二偶然看到的知乎问题：

如何用机器学习实现矩阵求逆，有什么思路？

- 输入：一个矩阵
- 输出：
  - 一个布尔值，表示输入矩阵是否存在逆
  - 一个矩阵，当输入矩阵存在逆时，为输入矩阵的逆

问题的建设性回答出现在2023年10月，来自南开的答主使用AI代码进行了一次训练实验，结果很不准确，且仅对2×2矩阵有效。随后指出：
1. 大模型对于矩阵求逆这类具有唯一正确解法的问题显然不适用
2. 大模型AI助手不能写出完美的程序，调试过程仍需人工把关（这一点我在后续深有体会）

更进一步，有大佬在评论区指出："关键在于'求逆不是Lipschitz连续'，这样的问题意味着微小的输入扰动会导致巨大的输出波动，这就要求模型的参数必须特别精确，而不能是大差不差。也就是说在整个模型参数的变化空间中可能只有一个地方是可接受的解，甚至是离这个解非常近的一组模型参数都是完全不可接受的。想象一下损失函数关于模型参数的landscape图，首先是这个landscape会非常崎岖，而且只有一个地方是可以接受的，这个要求非常苛刻；其次是这个landscape的梯度对模型完全不能起到指导作用，模型的下一次探索近乎是盲目的。"

那么，这一过程真的不可实现吗？继续查阅评论区，幸运的是，有用户提供了2022年10月发表在TMLR上的论文《Linear algebra with transformers》（没想到真有人有这个想法，学术界真的好无聊hh），其中详细列出了作者针对矩阵计算模型训练的探索。对于15维以下的矩阵9种运算，在最坏情况下准确率也达到了85%，同时32页的论文里详细列出了多种对矩阵输入的编码方式、训练所用的解码器和编码器层数，以及训练用时的实验数据，显然具有可行性。

## 项目准备

- **硬件条件**：4060笔记本一台，轻薄本一台，均装有Linux系统以及Anaconda中的PyTorch环境若干
- **人员配置**：cs初学者一名（本人）
- **前置知识**：C和C++基础语法，Python阅读无大碍，Cursor Coder，零散的CS知识若干
- **项目目标**：实现由大模型推理为内核的10维以下可变矩阵计算器

## 项目进程

### 周二

这是周二中午一点，安排项目规划：训练一个可用的10维以下矩阵模型计算器，9个模型分别训练。AI给出的训练时间预测：H100预计总用时8到9小时，4060预计5到7天，因此选择云服务器在所难免。

下午正好没课，一直到睡觉前，了解云算力平台，从算力云到阿里云，最后选择了性价比最高的AutoDL 4090显卡，预算150元进行部署。

### 周三

上午：部署成功，成为"一品丹道师"，学习实例和镜像选择、版本兼容问题等。

中午1点：根据论文方法，正式开始第一次"炼丹"（训练模型），看着进度条一点点前进，loss一点点减小，充满了成就感。

晚上：第一次训练完成，经过测试，全部失败（准确度0%），绷不住了。经过检查，发现AI在训练代码中使用了Decoder-only，与2022年论文中使用的encoder and decoder方法冲突。若采用论文中的训练模式，即使是4090，也需要7天，这个成本不是经济拮据的大学生可以承受的。这也是第一次被AI误导。

### 周四

上午：进入论文对应的代码仓库（www.github.com/facebookresearch/LAWT），决定直接采用其中的预训练模型和相关src文件进行复现，并决定将计算器部署在云服务器，实现点击链接即可访问使用，从而从大作业文件形式转向更通用的网页形式。

下午：发现DeepSeek以及豆包的代码生成局限性逐渐显现，换用通义千问3MAX付费模型（充值了10元，甚至还收到了阿里云客服的电话），一整天基本都在复制粘贴，但总是调试不好，本地服务无法使用。

晚上：对话式大模型已经捉襟见肘，决定使用更强大的AI IDE——Cursor。很遗憾Cursor不允许国内用户注册，为此使用了一些特殊手段，获得了7天体验券。

第一次使用Cursor果然效果非常惊艳，不亚于高中时第一次使用DeepSeek（大概在其爆火的半个月前），自动接管命令行、修改代码文件、兼容VSCode模式、可联网访问GitHub等功能无比强大，约一小时就成功开发了第一版前端页面。登录到自己开发的网址，看到计算器页面出现时，感觉一切都有了意义。

这天晚上的小插曲：小红书上刷到北邮学长大三获得青云计划，深受触动：从未想过自己能达到青云的标准（即使读博），更不敢想本科就可以进入。有些人在大一就可以解决手撕字节hard，有些人却还对着AI写的代码束手无策，不禁感慨万千。（此事还有后续，详见[大厂学长交流](../month3-main#L48-L50)）

### 周五

上午：连接模型，开发功能，测试。

下午：计算机导论和班会课，不得不暂时离开心爱的项目，重拾数据结构。

晚上：基本收工，测试网页链接是否可用。

### 周六

最终测试一直持续到中午一点，舍友提出了一些动画改进建议，于是添加了一些前端动画（第二版），优化服务器，产生了最理想的版本，随后删除本地和云服务器缓存等历史文件。

然后噩梦发生了，Cursor删除历史文件时影响了原有代码，导致模型计算自动回退到NumPy结果。进一步检查发现模型解码错误、字典错误。反复修改，没有任何办法回退到之前的版本（血泪教训：代码一定要及时上传到Git）。

晚上：反复修改、重启、上传、测试，全部宣告失败，且在这一过程中逐渐对AI自动代码生成祛魅。即使反复提醒，AI总是莫名进入死循环。

### 周日

上午：确实没想到最终做到了这个地步，今天必须结项。

于是全部重来，从头开始构建前端页面和模型调用，从头开始测试，并且加入了评论、用户数量显示等辅助功能，最终做出大家看到的这个版本，可供访问。

晚上：撰写日志以作纪念。

## 未来展望

以下是几条补充信息：

1. AI目前距离取代程序员还很远，无论是在项目结构还是具体操作方面（亲身体会，目前为止程序员尚不至于失业）
2. 使用AI可以大大增强个人开发者的能力，每个想法都有实现的可能
3. 目前这个版本的LAWT计算模式确实会调用神经网络进行计算，但有一定概率回退到NumPy计算，或许要等到来日我深入了解Transformers后再来解决
4. 欢迎进一步交流了解详情，本项目的代码经整理后也会上传到GitHub上，欢迎关注我的GitHub账号wsgbxx，点个小星星～